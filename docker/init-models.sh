#!/bin/bash
# docker/init-models.sh

set -e  # Parar em caso de erro

echo "ğŸš€ Iniciando configuraÃ§Ã£o do ambiente..."

# Verificar se Ollama estÃ¡ funcionando
echo "ğŸ“¡ Verificando Ollama..."
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama nÃ£o encontrado!"
    exit 1
fi

# Iniciar serviÃ§o Ollama em background
echo "ğŸ”„ Iniciando serviÃ§o Ollama..."
ollama serve &
OLLAMA_PID=$!

# Aguardar Ollama estar pronto
echo "â³ Aguardando Ollama ficar disponÃ­vel..."
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama estÃ¡ pronto!"
        break
    fi
    if [ $i -eq 30 ]; then
        echo "âŒ Timeout aguardando Ollama"
        exit 1
    fi
    sleep 2
done

# Baixar modelos necessÃ¡rios
echo "ğŸ“¥ Baixando modelo DeepSeek Coder 1.3B..."
if ! ollama pull deepseek-coder:1.3b; then
    echo "âŒ Falha ao baixar deepseek-coder:1.3b"
    # Continuar mesmo se um modelo falhar
fi

echo "ğŸ“¥ Baixando modelo CodeLlama 7B..."
if ! ollama pull codellama:7b; then
    echo "âŒ Falha ao baixar codellama:7b"
    # Continuar mesmo se um modelo falhar
fi

# Verificar modelos instalados
echo "ğŸ“‹ Modelos disponÃ­veis:"
ollama list

# Testar modelos bÃ¡sicos
echo "ğŸ§ª Testando DeepSeek..."
if ollama run deepseek-coder:1.3b "print('hello')" 2>/dev/null | head -1; then
    echo "âœ… DeepSeek funcionando"
else
    echo "âš ï¸ DeepSeek pode ter problemas"
fi

echo "ğŸ§ª Testando CodeLlama..."
if ollama run codellama:7b "print('hello')" 2>/dev/null | head -1; then
    echo "âœ… CodeLlama funcionando"  
else
    echo "âš ï¸ CodeLlama pode ter problemas"
fi

echo "ğŸ‰ ConfiguraÃ§Ã£o concluÃ­da!"
echo "ğŸ“Š Execute: docker-compose exec analysis python scripts/calculate_metrics.py dataset/juice_shop_15_files.csv"

# Manter container rodando
tail -f /dev/null