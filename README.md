
# Avalia√ß√£o Comparativa do Desempenho de Intelig√™ncias Artificiais Generativas e Ferramentas Tradicionais na An√°lise de C√≥digo-Fonte JavaScript

**Artigo:** "Avalia√ß√£o comparativa do desempenho de intelig√™ncias artificiais generativas e ferramentas tradicionais na an√°lise de c√≥digo-fonte JavaScript"

**Resumo do Artefato:** Este artefato implementa uma compara√ß√£o sistem√°tica do desempenho de intelig√™ncias artificiais generativas e ferramentas tradicionais na an√°lise de c√≥digo-fonte JavaScript. Ele permite a replica√ß√£o dos experimentos que avaliaram ferramentas SAST (Semgrep/SonarQube) e modelos Large Language Models (LLMs) como DeepSeek e CodeLlama na detec√ß√£o de vulnerabilidades JavaScript utilizando o OWASP Juice Shop como dataset. O estudo revela que as ferramentas SAST alcan√ßam 100% de precis√£o para vulnerabilidades padr√£o (XSS/SQLi) , enquanto os LLMs oferecem maior recall (70% no DeepSeek) para amea√ßas contextuais (NoSQLi/controle de acesso quebrado), demonstrando a complementaridade entre as abordagens.

## Estrutura do readme.md

```
‚îú‚îÄ‚îÄ .github/workflows/                           # Pipelines SAST (GitHub Actions)
‚îÇ   ‚îú‚îÄ‚îÄ semgrep.yml
‚îÇ   ‚îî‚îÄ‚îÄ sonarqube.yml
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ code_snippets/                           # Trechos de c√≥digo analisados (15 arquivos .ts)
‚îÇ   ‚îú‚îÄ‚îÄ juice_shop_15_files.csv                  # Metadados das an√°lises e ground truth
‚îÇ   ‚îú‚îÄ‚îÄ templates/                               # Modelos de arquivos para preenchimento manual
‚îÇ       ‚îú‚îÄ‚îÄ llm_detections_manual_template.csv   # Template para registro manual das detec√ß√µes dos LLMs
‚îú‚îÄ‚îÄ results/                                     # Armazena os resultados gerados pelos experimentos. Esta pasta inicia vazia e √© preenchida pelos scripts.
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ calculate_metrics.py    # Gera a Tabela 2 do artigo
‚îÇ   ‚îú‚îÄ‚îÄ run_llm_analysis.py     # Executa an√°lise com LLMs (atualizado)
‚îÇ   ‚îú‚îÄ‚îÄ run_sonar_analysis.sh   # An√°lise SonarQube automatizada (NOVO)
‚îÇ   ‚îú‚îÄ‚îÄ validate_contextual_recall.py
‚îÇ   ‚îú‚îÄ‚îÄ validate_environment.sh # Valida√ß√£o completa do ambiente (86 testes)
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python com vers√µes travadas
‚îú‚îÄ‚îÄ Makefile                    # Interface de comandos simplificada 
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestra√ß√£o de servi√ßos (Ollama, SonarQube, etc.)
‚îî‚îÄ‚îÄ README.md                   # Este arquivo
```

## Selos Considerados

Os selos considerados para este artefato s√£o:

  * **Artefatos Dispon√≠veis (Selo D):** O c√≥digo-fonte, os scripts de automa√ß√£o e os dados (dataset de arquivos e ground truth) est√£o publicados em um reposit√≥rio p√∫blico no GitHub.
  * **Artefatos Funcionais (Selo F):** O artefato inclui instru√ß√µes claras e scripts para a instala√ß√£o e execu√ß√£o das ferramentas, permitindo que o revisor observe as funcionalidades de an√°lise de vulnerabilidades tanto das ferramentas SAST quanto dos LLMs. Um teste m√≠nimo com sa√≠da esperada √© fornecido.
  * **Artefatos Sustent√°veis (Selo S):** As depend√™ncias s√£o versionadas atrav√©s de `requirements.txt`, o ambiente √© documentado, e a estrutura do c√≥digo √© organizada para facilitar a compreens√£o e manuten√ß√£o.
  * **Experimentos Reprodut√≠veis (Selo R):** Os experimentos, incluindo o c√°lculo das m√©tricas comparativas e a demonstra√ß√£o da an√°lise de LLMs e SASTs, podem ser reproduzidos a partir de instru√ß√µes passo a passo e scripts, com a indica√ß√£o das sa√≠das esperadas.

## Informa√ß√µes b√°sicas

### Ambiente de Execu√ß√£o

  * **Sistema Operacional:** Linux (Ubuntu 20.04+), macOS ou WSL2
  * **Python:** 3.11
  * **Docker:** 20.10 (necess√°rio para a execu√ß√£o local do SonarQube, se optar por n√£o usar GitHub Actions)
  * **Mem√≥ria RAM:** M√≠nimo 4GB (8GB recomendado para a execu√ß√£o dos LLMs).
  * **Espa√ßo em disco:** 5GB livres.

### Recursos Adicionais

  * **Acesso √† Internet:** Necess√°rio para o download de depend√™ncias (modelos Ollama, pacotes Python, etc.).
  * **GPU:** Opcional para acelera√ß√£o da infer√™ncia dos LLMs (altamente recomendado para reduzir o tempo de execu√ß√£o).



## Dataset Base

  * **OWASP Juice Shop v17.3.0:** O artefato utiliza um subconjunto de 15 arquivos desta aplica√ß√£o web intencionalmente vulner√°vel.
  * **Arquivos de c√≥digo-fonte:** Localizados em `dataset/code_snippets/`. Estes arquivos incluem exemplos vulner√°veis e seguros, alguns dos quais foram pr√©-processados (remo√ß√£o de coment√°rios de desafio) para a an√°lise dos LLMs, conforme descrito no artigo.
  * **Ground Truth:** O arquivo `dataset/juice_shop_15_files.csv` cont√©m o mapeamento das vulnerabilidades conhecidas para cada arquivo, servindo como a "verdade" para o c√°lculo das m√©tricas de desempenho.

## üîÑ Notas sobre Reprodutibilidade dos LLMs

Resultados de modelos generativos (DeepSeek/CodeLlama) **podem variar entre execu√ß√µes** devido √†:

- Natureza probabil√≠stica dos modelos
- Sensibilidade contextual em JavaScript
- Limita√ß√µes de janela de contexto
- Varia√ß√µes na inicializa√ß√£o de pesos

## Instala√ß√£o

Siga os passos abaixo para configurar o ambiente e as ferramentas:

### 1\. Clonagem do Reposit√≥rio

```bash
git clone https://github.com/rayanepimentel/avaliacao-comparativa-sast-llm.git
cd avaliacao-comparativa-sast-llm
```

### 2\.Execute ambiente completo

```bash
make quick-start
```
A execu√ß√£o do `make quick-start` pode levar de 15 a 40 minutos, pois inclui o download dos modelos de linguagem.

## Teste M√≠nimo

Ao rodar `make quick-start`, ser√° gerado o teste m√≠mino.

**Sa√≠da esperada no terminal (similar √† Tabela 2 do artigo):**

**Tabela 2. Compara√ß√£o de M√©tricas entre Ferramentas de An√°lise de Seguran√ßa**

| Ferramenta | Precis√£o | Recall | F1-Score | FP Rate | VP | FP | FN |
|------------|----------|--------|----------|---------|----|----|----|
| Semgrep    | 100%     | 50%    | 67%      | 0%      | 5  | 0  | 5  |
| SonarQube  | 100%     | 10%    | 18%      | 0%      | 1  | 0  | 9  |
| DeepSeek   | 78%      | 70%    | 74%      | 22%     | 7  | 2  | 3  |
| CodeLlama  | 55%      | 60%    | 57%      | 45%     | 6  | 5  | 4  |

> **Nota:** VP = Verdadeiros Positivos, FP = Falsos Positivos, FN = Falsos Negativos.  
> Dados obtidos da an√°lise do OWASP Juice Shop v17.3.0.


**Arquivos gerados/atualizados na pasta `results/` (inicialmente vazia):**

* `results/metrics_table.csv`: Um arquivo CSV contendo as m√©tricas detalhadas para cada ferramenta.
* `results/metrics_table.html`: Um arquivo HTML com a Tabela 2 formatada.

### Valida√ß√£o da Execu√ß√£o

Para validar que o artefato foi corretamente instalado e executado:

1. Verifique a presen√ßa dos arquivos `metrics_table.csv` e `metrics_table.html` na pasta `results/`.
2. Abra o HTML no navegador e compare com a Tabela 2 do artigo.
3. Os tempos de execu√ß√£o devem estar pr√≥ximos dos relatados.


## Experimentos

Esta se√ß√£o descreve os passos para reproduzir os resultados apresentados no artigo.


### Reivindica√ß√£o \#1: Recall de LLMs em Vulnerabilidades Contextuais

**Objetivo**: Validar que os modelos DeepSeek e CodeLlama alcan√ßam alto recall em amea√ßas complexas e contextuais (como NoSQLi e Broken Access Control), compreendendo a natureza de suas detec√ß√µes e a variabilidade inerente a modelos generativos.

**Processo**:
Este processo √© dividido em duas etapas e deve ser executado na sequ√™ncia:

1.  **Execu√ß√£o da An√°lise Bruta de LLMs (`run_llm_analysis.py`)**:

  ```bash
    make test-llm
   ```
 **Registre a Detec√ß√£o e a Categoria Identificada:**
 O script `run_llm_analysis.py` gera automaticamente o arquivo `results/llm_detections_results.csv`. 
 
 Caso queira revisar ou preencher os dados manualmente, utilize o template de planilha fornecido em `dataset/templates/llm_detections_manual_template.csv`.

 - Se a resposta do LLM for `C√≥digo seguro`, registre a detec√ß√£o como **0** (n√£o vulner√°vel) e a categoria como `N/A`.
 - Se a resposta for um JSON com detalhes da vulnerabilidade, registre a 

2.  **C√°lculo do Recall Contextual `(validate_contextual_recall.py)`**:

Com o arquivo `results/llm_detections_results.csv` gerado (automaticamente ou manualmente), execute o script `validate_contextual_recall.py`. Este script calcular√° o recall dos LLMs especificamente para as vulnerabilidades contextuais.

```Bash
make validate
```

- Tempo esperado: Menos de 10 segundos.
- Recursos esperados: Baixo consumo de CPU/RAM.

Sa√≠da Esperada no terminal:

üîç **Resultados para Vulnerabilidades Contextuais:**

- **Amostras analisadas:** XY  
- **DeepSeek:** 4 detectadas | **Recall:** Y.Y% 
- **CodeLlama:** 3 detectadas | **Recall:** X.X%

**Detalhes por vulnerabilidade:**

| ID       | Vulnerability             | Detected_Deepseek | Detected_CodeLlama |
|----------|---------------------------|-------------------|--------------------|
| VULN-04  | NoSQL Injection           | X                 | X                  |
| VULN-05  | Broken Access Control     | X                 | X                  |
| VULN-06  | Sensitive Data Exposure   | X                 | X                  |
| VULN-09  | Broken Access Control     | X                 | X                  |
| VULN-10  | Valida√ß√£o insuficiente    | X                 | X                  |


Onde XY √© o n√∫mero de amostras contextuais, Y.Y% e X.X% s√£o os recalls calculados para DeepSeek e CodeLlama, e X na tabela ser√° 1 ou 0, indicando se o LLM detectou uma vulnerabilidade (1) ou n√£o (0) naquela amostra.


### Reivindica√ß√£o \#2: Execu√ß√£o SAST 

Execute

```bash
make test-sast
```

Verifique a sa√≠da no terminal e na pasta `/results/`

- semgrep_results.json
- semgrep_results.sarif
- sonarqube.issues.json


## Interface de Comandos Simplificada

Use o `Makefile` para comandos mais simples:

```bash
# Setup completo (recomendado para iniciantes)
make quick-start

# Comandos individuais
make test-minimal    # Teste r√°pido (2 min) - gera Tabela 2
make test-llm        # An√°lise com IA (30-80 min)
make test-sast       # Ferramentas profissionais (5-10 min)

# Gerenciamento
make logs           # Ver logs em tempo real
make shell          # Acessar terminal interno

# Solu√ß√£o de problemas
make reinit-models  # Se os modelos LLM falharem
make restart-sonarqube # Se o SonarQube travar
make clean          # Limpar tudo e recome√ßar

# Valida√ß√£o
make validate-env   # 86 verifica√ß√µes do ambiente
make validate       # Valida√ß√£o completa
```

## LICENSE

MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

