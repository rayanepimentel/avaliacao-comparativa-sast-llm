
# Avalia√ß√£o Comparativa do Desempenho de Intelig√™ncias Artificiais Generativas e Ferramentas Tradicionais na An√°lise de C√≥digo-Fonte JavaScript

**Artigo:** "Avalia√ß√£o comparativa do desempenho de intelig√™ncias artificiais generativas e ferramentas tradicionais na an√°lise de c√≥digo-fonte JavaScript"

**Resumo do Artefato:** Este artefato implementa uma compara√ß√£o sistem√°tica do desempenho de intelig√™ncias artificiais generativas e ferramentas tradicionais na an√°lise de c√≥digo-fonte JavaScript. Ele permite a replica√ß√£o dos experimentos que avaliaram ferramentas SAST (Semgrep/SonarQube) e modelos Large Language Models (LLMs) como DeepSeek e CodeLlama na detec√ß√£o de vulnerabilidades JavaScript utilizando o OWASP Juice Shop como dataset. O estudo revela que as ferramentas SAST alcan√ßam 100% de precis√£o para vulnerabilidades padr√£o (XSS/SQLi) , enquanto os LLMs oferecem maior recall (70% no DeepSeek) para amea√ßas contextuais (NoSQLi/controle de acesso quebrado), demonstrando a complementaridade entre as abordagens.

## Estrutura do readme.md

```
‚îú‚îÄ‚îÄ .github/workflows/                # Pipelines SAST (GitHub Actions)
‚îÇ   ‚îú‚îÄ‚îÄ semgrep.yml
‚îÇ   ‚îî‚îÄ‚îÄ sonarqube.yml
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ code_snippets/                # Trechos de c√≥digo analisados (15 arquivos .ts)
‚îÇ   ‚îú‚îÄ‚îÄ juice_shop_15_files.csv       # Metadados das an√°lises e ground truth
‚îú‚îÄ‚îÄ results/                          # Armazena os resultados gerados pelos experimentos. Esta pasta inicia vazia e √© preenchida pelos scripts.
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ calculate_metrics.py          # Gera a Tabela 2 do artigo
‚îÇ   ‚îú‚îÄ‚îÄ llm_calculate_metrics.py      # Gera calculo de m√©tricas LLMs
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt              # Depend√™ncias Python
‚îÇ   ‚îú‚îÄ‚îÄ run_llm_analysis.py           # Executa an√°lise com LLMs
‚îÇ   ‚îú‚îÄ‚îÄ setup_models.sh               # Instala Ollama + modelos
‚îî‚îÄ‚îÄ README.md                         # Este arquivo
```

## Selos Considerados

Os selos considerados para este artefato s√£o:

  * **Artefatos Dispon√≠veis (Selo D):** O c√≥digo-fonte, os scripts de automa√ß√£o e os dados (dataset de arquivos e ground truth) est√£o publicados em um reposit√≥rio p√∫blico no GitHub.
  * **Artefatos Funcionais (Selo F):** O artefato inclui instru√ß√µes claras e scripts para a instala√ß√£o e execu√ß√£o das ferramentas, permitindo que o revisor observe as funcionalidades de an√°lise de vulnerabilidades tanto das ferramentas SAST quanto dos LLMs. Um teste m√≠nimo com sa√≠da esperada √© fornecido.
  * **Artefatos Sustent√°veis (Selo S):** As depend√™ncias s√£o versionadas atrav√©s de `requirements.txt`, o ambiente √© documentado, e a estrutura do c√≥digo √© organizada para facilitar a compreens√£o e manuten√ß√£o.
  * **Experimentos Reprodut√≠veis (Selo R):** Os experimentos, incluindo o c√°lculo das m√©tricas comparativas e a demonstra√ß√£o da an√°lise de LLMs e SASTs, podem ser reproduzidos a partir de instru√ß√µes passo a passo e scripts, com a indica√ß√£o das sa√≠das esperadas.

## Informa√ß√µes b√°sicas

### Ambiente de Execu√ß√£o

  * **Sistema Operacional:** Linux (Ubuntu 20.04+), macOS ou WSL2
  * **Python:** 3.8+ (Recomendado: Python 3.9, 3.10, 3.11 ou 3.12 para melhor compatibilidade com as bibliotecas cient√≠ficas como scikit-learn)
  * **Docker:** 20.10+ (necess√°rio para a execu√ß√£o local do SonarQube, se optar por n√£o usar GitHub Actions)
  * **Mem√≥ria RAM:** M√≠nimo 4GB (8GB recomendado para a execu√ß√£o dos LLMs).
  * **Espa√ßo em disco:** 5GB livres.

### Recursos Adicionais

  * **Acesso √† Internet:** Necess√°rio para o download de depend√™ncias (modelos Ollama, pacotes Python, etc.).
  * **GPU:** Opcional para acelera√ß√£o da infer√™ncia dos LLMs (altamente recomendado para reduzir o tempo de execu√ß√£o).

## Depend√™ncias

### Essenciais (para reprodu√ß√£o b√°sica e c√°lculo de m√©tricas)

```bash
# Bibliotecas Python
pandas==2.0.3
scikit-learn==1.3.2
numpy==1.26.4
ollama==0.5.1 
python-dotenv==1.0.1 # Para carregar vari√°veis de ambiente
```

### Opcionais (para an√°lise completa via Ollama e SAST)

```bash
# Para LLMs (Ollama e seus modelos)
ollama>=0.5.1 # Vers√£o utilizada no desenvolvimento
# Os modelos CodeLlama-7b e DeepSeek-Coder-1.3b s√£o gerenciados pelo Ollama.

# Para SASTs (via GitHub Actions ou Docker para SonarQube)
# As ferramentas Semgrep (vers√£o 1.49) e SonarQube (Community Build vers√£o 25.5)
# s√£o instaladas e gerenciadas via GitHub Actions ou localmente via Docker.
```

## Dataset Base

  * **OWASP Juice Shop v17.3.0:** O artefato utiliza um subconjunto de 15 arquivos desta aplica√ß√£o web intencionalmente vulner√°vel.
  * **Arquivos de c√≥digo-fonte:** Localizados em `dataset/code_snippets/`. Estes arquivos incluem exemplos vulner√°veis e seguros, alguns dos quais foram pr√©-processados (remo√ß√£o de coment√°rios de desafio) para a an√°lise dos LLMs, conforme descrito no artigo.
  * **Ground Truth:** O arquivo `dataset/juice_shop_15_files.csv` cont√©m o mapeamento das vulnerabilidades conhecidas para cada arquivo, servindo como a "verdade" para o c√°lculo das m√©tricas de desempenho.

## Preocupa√ß√µes com seguran√ßa

‚ö†Ô∏è **ATEN√á√ÉO:** Este artefato utiliza c√≥digo intencionalmente vulner√°vel proveniente do OWASP Juice Shop para fins de pesquisa e demonstra√ß√£o. N√£o utilize este c√≥digo ou o ambiente de teste em sistemas de produ√ß√£o ou em redes n√£o seguras. Recomenda-se a execu√ß√£o em um ambiente isolado, como uma m√°quina virtual.

## üîÑ Notas sobre Reprodutibilidade dos LLMs

Resultados de modelos generativos (DeepSeek/CodeLlama) **podem variar entre execu√ß√µes** devido √†:

- Natureza probabil√≠stica dos modelos
- Sensibilidade contextual em JavaScript
- Limita√ß√µes de janela de contexto
- Varia√ß√µes na inicializa√ß√£o de pesos

Para comparativos acad√™micos, em [Reivindica√ß√£o #1](#reivindica√ß√£o-1-m√©tricas-comparativas-de-desempenho---llms) execute o script `run_llm_analysis.py` **3 vezes** e utilize a mediana dos resultados.

## Instala√ß√£o

Siga os passos abaixo para configurar o ambiente e as ferramentas:

### 1\. Clonagem do Reposit√≥rio

```bash
git clone https://github.com/rayanepimentel/avaliacao-comparativa-sast-llm.git
cd avaliacao-comparativa-sast-llm
```

### 2\. Ambiente Python

Crie um ambiente virtual e instale as depend√™ncias Python:

```bash
python3 -m venv venv
source venv/bin/activate  # Para Linux/macOS
# ou
.\venv\Scripts\activate   # Para Windows (no PowerShell)

# Instale as depend√™ncias 
pip install -r scripts/requirements.txt
```

### 3\. Instala√ß√£o do Ollama e Modelos LLM (Opcional, mas necess√°rio para an√°lise LLM)

Este passo instala a ferramenta Ollama e baixa os modelos CodeLlama e DeepSeek-Coder.

```bash
# Dar permiss√£o de execu√ß√£o e executar o script de instala√ß√£o
chmod +x scripts/setup_models.sh
./scripts/setup_models.sh
```

*Este script automatiza:*

  * A instala√ß√£o do Ollama para o seu sistema operacional.
  * O download dos modelos `codellama:7b` e `deepseek-coder:1.3b` via Ollama.


## Teste M√≠nimo

Execute o script de c√°lculo de m√©tricas para verificar a funcionalidade b√°sica do ambiente e a gera√ß√£o de resultados com o dataset original (juice_shop_15_files.csv). Este teste validar√° a correta instala√ß√£o das depend√™ncias Python e a execu√ß√£o do calculate_metrics.py.

```bash
python scripts/calculate_metrics.py dataset/juice_shop_15_files.csv
```

**Sa√≠da esperada no terminal (similar √† Tabela 2 do artigo):**

**Tabela 2. Compara√ß√£o de M√©tricas entre Ferramentas de An√°lise de Seguran√ßa**

| Ferramenta | Precis√£o | Recall | F1-Score | FP Rate | VP | FP | FN |
|------------|----------|--------|----------|---------|----|----|----|
| Semgrep    | 100%     | 50%    | 67%      | 0%      | 5  | 0  | 5  |
| SonarQube  | 100%     | 10%    | 18%      | 0%      | 1  | 0  | 9  |
| DeepSeek   | 78%      | 70%    | 74%      | 22%     | 7  | 2  | 3  |
| CodeLlama  | 55%      | 60%    | 57%      | 45%     | 6  | 5  | 4  |

> **Nota:** VP = Verdadeiros Positivos, FP = Falsos Positivos, FN = Falsos Negativos.  
> Dados obtidos da an√°lise do OWASP Juice Shop v17.3.0.


**Arquivos gerados/atualizados na pasta `results/` (inicialmente vazia):**

* `results/metrics_table.csv`: Um arquivo CSV contendo as m√©tricas detalhadas para cada ferramenta.
* `results/metrics_table.html`: Um arquivo HTML com a Tabela 2 formatada.

### Valida√ß√£o da Execu√ß√£o

Para validar que o artefato foi corretamente instalado e executado:

1. Verifique a presen√ßa dos arquivos `metrics_table.csv` e `metrics_table.html` na pasta `results/`.
2. Abra o HTML no navegador e compare com a Tabela 2 do artigo.
3. Os tempos de execu√ß√£o devem estar pr√≥ximos dos relatados.


## Experimentos

Esta se√ß√£o descreve os passos para reproduzir os resultados apresentados no artigo.


### Reivindica√ß√£o \#1: An√°lise de Desempenho de LLMs com Valida√ß√£o de Categoria

**Objetivo**: Reproduzir as m√©tricas de precis√£o, recall, F1-score e taxa de falsos positivos **especificamente para DeepSeek e CodeLlama**, aplicando uma valida√ß√£o rigorosa da categoria da vulnerabilidade identificada.

**Processo**:
Este processo √© dividido em duas etapas e deve ser executado na sequ√™ncia:

1.  **Execu√ß√£o da An√°lise Bruta de LLMs (`run_llm_analysis.py`):** O script `run_llm_analysis.py` interage com os modelos Ollama, analisa cada trecho de c√≥digo do dataset e consolida as respostas das detec√ß√µes dos LLMs em um novo arquivo CSV na pasta `results/`.

    ```bash
    python scripts/run_llm_analysis.py
    ```

      * **Tempo esperado:** Aproximadamente 30-45 minutos (em CPU com 8GB RAM). Pode ser significativamente mais r√°pido com GPU.
      * **Recursos esperados:** Uso intensivo de CPU e RAM durante a infer√™ncia dos LLMs.
      * **Sa√≠da:** Um arquivo `results/llm_detections_results.csv` ser√° gerado, contendo os IDs dos arquivos, caminhos, detec√ß√µes bin√°rias iniciais (0 ou 1, baseadas na *presen√ßa de qualquer vulnerabilidade*), tempos de execu√ß√£o e as respostas brutas dos LLMs (textos ou JSONs). Logs de execu√ß√£o e progresso ser√£o exibidos no terminal.

    **Alternativa Manual (se `run_llm_analysis.py` falhar):**
    Caso a execu√ß√£o automatizada do `run_llm_analysis.py` encontre problemas operacionais dentro do cont√™iner, √© poss√≠vel realizar a an√°lise dos LLMs manualmente, interagindo diretamente com o Ollama CLI. Este processo deve ser repetido para cada um dos 15 arquivos de c√≥digo-fonte no diret√≥rio `/dataset/code_snippets/` e para cada modelo LLM (DeepSeek-Coder:1.3b e CodeLlama:7b).

    **Passos para An√°lise Manual:**
    a.  **Inicie o Ollama com o modelo desejado no terminal:**

    ```bash 
    ollama run deepseek-coder:1.3b # Ou ollama run codellama:7b 
    ```

    Voc√™ ver√° o prompt `>>> Send a message (/? for help)`.

    b.  **Construa o Prompt:** Copie o template de prompt abaixo e cole no terminal do Ollama.
    \`\`\`text
    Analise os riscos de seguran√ßa no c√≥digo abaixo, seguindo o OWASP Top 10. Retorne APENAS se houver vulnerabilidades, no formato JSON abaixo. Caso contr√°rio, retorne "C√≥digo seguro".

    ````
    {
      "Arquivo": "Nome do arquivo",
      "Trecho Vulner√°vel": "O snippet de c√≥digo espec√≠fico que cont√©m a vulnerabilidade.",
      "Tipo da Vulnerabilidade": "A categoria da vulnerabilidade",
      "Descri√ß√£o Breve": "Uma explica√ß√£o concisa (1-2 frases) de por que esse trecho √© vulner√°vel"
    }

    Code:
    ```typescript
    # COLE AQUI O CONTE√öDO DO ARQUIVO DE C√ìDIGO
    ```
    ```
    ````

    c.  **Obtenha o Conte√∫do do C√≥digo:** V√° para o diret√≥rio `/dataset/code_snippets/` dentro do cont√™iner, abra um dos 15 arquivos (ex: `VULN-01.ts` ou `SAFE-01.ts`), copie todo o conte√∫do e cole-o no prompt do Ollama, substituindo `# COLE AQUI O CONTE√öDO DO ARQUIVO DE C√ìDIGO`.

    d.  **Envie e Colete a Resposta:** Pressione `Enter` para enviar o prompt ao LLM. Copie a resposta completa gerada pelo Ollama.

    e.  **Registre a Detec√ß√£o e a Categoria Identificada:**
    - Se a resposta do LLM for `C√≥digo seguro`, registre a detec√ß√£o como **0** (n√£o vulner√°vel) e a categoria como `N/A`.
    - Se a resposta for um JSON com detalhes da vulnerabilidade, registre a detec√ß√£o como **1** (vulner√°vel) e extraia a **"Tipo da Vulnerabilidade"** (`Vulnerability Category`) do JSON ‚Äì Verifique se o tipo de vulnerabilidade identificada foi a categoria correta. Se o LLM descrever a vulnerabilidade em texto livre (sem JSON), tente identificar a categoria principal mencionada no texto.
    - Mantenha um registro em uma planilha ou similar (por exemplo, `llm_detections_manual.csv`) dos IDs dos arquivos, as detec√ß√µes (0 ou 1), e a **categoria da vulnerabilidade identificada pelo LLM**. Isso √© crucial para a pr√≥xima etapa.

    f.  **Repita:** Repita os passos de `a` a `e` para todos os 15 arquivos do dataset e para ambos os modelos LLM (DeepSeek-Coder:1.3b e CodeLlama:7b).

    g.  **Crie o `llm_detections_results.csv` manual:** 
    Ap√≥s coletar todas as detec√ß√µes e categorias, crie manualmente um arquivo `/results/llm_detections_results.csv` com as colunas `ID`, `File`, `Vulnerability`, `Detected_Deepseek` (0 ou 1), `DeepSeek_Raw_Result`, `DeepSeek_Time`, `CodeLlama_Detected` (0 ou 1), `CodeLlama_Raw_Result`, `CodeLlama_Time`, **`DeepSeek_Identified_Category`** (NOVA COLUNA), e **`CodeLlama_Identified_Category`** (NOVA COLUNA). As colunas `*_Raw_Result` conter√£o a resposta bruta do LLM e `*_Time` pode ser 0.0, a menos que voc√™ as cronometre manualmente.

2.  **C√°lculo das M√©tricas de LLMs com Valida√ß√£o de Categoria (`llm_category_metrics.py`):**
    Uma vez que o arquivo `results/llm_detections_results.csv` (seja gerado automaticamente ou criado manualmente) esteja dispon√≠vel, execute o novo script `llm_category_metrics.py`. Este script:

      * Ler√° o `results/llm_detections_results.csv` (com as detec√ß√µes e categorias identificadas pelos LLMs).
      * Aplicar√° a l√≥gica de valida√ß√£o de categoria para contar VP/FP/FN.
      * Gerar√° uma tabela de m√©tricas **APENAS para DeepSeek e CodeLlama**.

    <!-- end list -->

    ```bash
    python scripts/llm_category_metrics.py
    ```

      * **Tempo esperado:** Menos de 10 segundos.
      * **Recursos esperados:** Baixo consumo de CPU/RAM.
      * **Sa√≠da:** As m√©tricas de desempenho de DeepSeek e CodeLlama (com valida√ß√£o de categoria) ser√£o impressas no terminal e salvas nos arquivos `/results/llm_category_metrics.csv` e `/results/llm_category_metrics.html`.



### Reivindica√ß√£o \#2: Execu√ß√£o SAST completa via GitHub Actions

Este processo demonstra como as an√°lises SAST foram integradas e executadas no ambiente de CI/CD (GitHub Actions) sobre o reposit√≥rio completo do OWASP Juice Shop.

1.  **Fa√ßa um fork do reposit√≥rio oficial do OWASP Juice Shop**:

      * Acesse `https://github.com/juice-shop/juice-shop`
      * Clique em "Fork" no canto superior direito para criar uma c√≥pia em sua conta.

2.  **Adicione os workflows SAST ao seu fork**:

      * No seu **fork** do reposit√≥rio `juice-shop`, navegue at√© o diret√≥rio `.github/workflows`.
      * Se n√£o existir, crie esta pasta.
      * Copie o conte√∫do dos arquivos `semgrep.yml` e `sonarqube.yml` deste projeto (localizados em `.github/workflows/`) para a pasta `.github/workflows/` no seu fork.

3.  **Configure os Secrets do GitHub (APENAS PARA SONARQUBE)**:

      * Para o workflow do SonarQube funcionar, voc√™ precisa configurar os segredos no seu fork do GitHub.
      * No seu fork, v√° em **Settings \> Secrets \> Actions \> New repository secret**.
      * Adicione dois segredos:
          * `SONAR_TOKEN`: Um token de autentica√ß√£o gerado na sua inst√¢ncia SonarQube (ou SonarCloud).
          * `SONAR_HOST_URL`: A URL da sua inst√¢ncia SonarQube (ex: `https://sonarcloud.io` ou `http://localhost:9000`).
      * Se encontrar problemas, consulte a [documenta√ß√£o de integra√ß√£o do SonarQube com GitHub Actions](https://docs.sonarqube.org/latest/analysis/github-integration/).

4.  **Execute os workflows**:

      * No seu fork do `juice-shop` no GitHub, v√° para a aba **Actions**.
      * No painel lateral esquerdo, selecione os workflows:
          * **Semgrep PR** (nome definido no `semgrep.yml`): Clique em "Run workflow" e selecione o branch principal (ou o branch que voc√™ adicionou os arquivos `.yml`).
          * **Sonar** (nome definido no `sonarqube.yml`): Clique em "Run workflow" e selecione o branch principal (ou o branch que voc√™ adicionou os arquivos `.yml`).

5.  **Obtenha os resultados**:

      * **Semgrep**: Ap√≥s a execu√ß√£o bem-sucedida do workflow "Semgrep PR", um novo Pull Request ser√° criado automaticamente em seu fork com o relat√≥rio de seguran√ßa do Semgrep. Acesse o PR para visualizar o relat√≥rio detalhado em Markdown.
      * **SonarQube**: Ap√≥s a execu√ß√£o bem-sucedida do workflow "Sonar", os resultados da an√°lise ser√£o enviados para a inst√¢ncia SonarQube configurada. Acesse a interface web da sua inst√¢ncia SonarQube (ex: `https://sonarcloud.io` ou sua URL local) e navegue at√© o projeto correspondente ao seu fork do Juice Shop para visualizar os alertas de seguran√ßa.
      *  Verifique se o tipo de vulnerabilidade identificada foi a categoria correta.

> **Notas importantes**:
>
>   * O tempo de execu√ß√£o varia: o Semgrep geralmente leva 2-5 minutos, enquanto o SonarQube pode levar de 5-15 minutos para concluir a an√°lise e enviar os resultados.
>   * A execu√ß√£o em GitHub Actions utiliza recursos do pr√≥prio GitHub; n√£o h√° consumo de recursos locais para o avaliador.


## LICENSE

MIT License

Copyright (c) 2025

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

